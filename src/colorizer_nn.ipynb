{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Import packages and device selection",
   "id": "b313fc0badf68a12"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import re\n",
    "import cv2\n",
    "from tqdm.notebook import tqdm\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, random_split, SubsetRandomSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchmetrics.functional.image import image_gradients, structural_similarity_index_measure\n",
    "from torchvision import transforms\n",
    "from torchinfo import summary\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "SIZE = 160\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ],
   "id": "2e23ac1ba4189527",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(\"Using device: \", device)",
   "id": "59ca34fe3c6ae5d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset Loading and Preprocessing",
   "id": "fbfc50c6b8689727"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def sort_files(folder):\n",
    "    convert_func = lambda x: int(x) if x.isdigit() else x.lower()\n",
    "    key_func = lambda x: [convert_func(c) for c in re.split('([0-9]+)', x)]\n",
    "    return sorted(folder, key=key_func)\n",
    "\n",
    "input_L = []\n",
    "target_ab = []\n",
    "path = '../data/color'\n",
    "folder = os.listdir(path)\n",
    "folder = sort_files(folder)\n",
    "for file in tqdm(folder, desc='Loading color images'):\n",
    "    img = cv2.imread(os.path.join(path, file), 1)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "    img = cv2.resize(img, (SIZE, SIZE))\n",
    "    L = img[:, :, 0:1] / 255.0 # (H, W, 1)\n",
    "    ab = img[:, :, 1:3] / 255.0 # (H, W, 2)\n",
    "    input_L.append(L)\n",
    "    target_ab.append(ab)\n",
    "input_L = np.array(input_L).astype(np.float32) # (N, H, W, 1)\n",
    "target_ab = np.array(target_ab).astype(np.float32) # (N, H, W, 2)"
   ],
   "id": "71ceb81e212e2ca7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(input_L.shape, target_ab.shape)",
   "id": "3f03936b33e6388b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Display some samples",
   "id": "e01565f8500bee4f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for _ in range(5):\n",
    "    idx = np.random.randint(0, len(input_L) - 1)\n",
    "    color_img = np.clip(np.concatenate([input_L[idx], target_ab[idx]], axis=2) * 255, 0, 255).astype(np.uint8) # (H, W, 3)\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title('RGB Color Image', fontsize=20)\n",
    "    plt.imshow(np.clip(cv2.cvtColor(color_img, cv2.COLOR_LAB2RGB), 0, 255).astype(np.uint8))\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title('Lab Color Image', fontsize=20)\n",
    "    plt.imshow(color_img)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.title('Grayscale Image ', fontsize=20)\n",
    "    plt.imshow((input_L[idx] * 255).astype(np.uint8).squeeze(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ],
   "id": "8c4b97681d3cd431",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Split into training and test data",
   "id": "e78f0f02e2584c76"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "input_L = np.transpose(input_L, (0, 3, 1, 2)) # (N, 1, H, W)\n",
    "target_ab = np.transpose(target_ab, (0, 3, 1, 2)) # (N, 2, H, W)\n",
    "L_train, L_test, ab_train, ab_test = train_test_split(input_L, target_ab, test_size=0.2, random_state=42)\n",
    "L_train = torch.tensor(L_train, dtype=torch.float32)\n",
    "ab_train = torch.tensor(ab_train, dtype=torch.float32)\n",
    "L_test = torch.tensor(L_test, dtype=torch.float32)\n",
    "ab_test = torch.tensor(ab_test, dtype=torch.float32)"
   ],
   "id": "f186b3490baaee72",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "L_mean = L_train.mean(dim=(0, 2, 3))\n",
    "L_std = L_train.std(dim=(0, 2, 3))\n",
    "ab_mean = ab_train.mean(dim=(0, 2, 3))\n",
    "ab_std = ab_train.std(dim=(0, 2, 3))"
   ],
   "id": "3fc3a89737df7119",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(L_mean, L_std)\n",
    "print(ab_mean, ab_std)"
   ],
   "id": "5d6ffe3602c4a41b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, L_data, ab_data, L_transform=None, ab_transform=None):\n",
    "        self.L_data = L_transform(L_data) if L_transform else L_data\n",
    "        self.ab_data = ab_transform(ab_data) if ab_transform else ab_data\n",
    "    def __len__(self):\n",
    "        return len(self.L_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.L_data[idx], self.ab_data[idx]"
   ],
   "id": "f15ddb4fe3b7a5af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training pipeline",
   "id": "5b1cc3045bc3a88a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "trainset = MyDataset(L_train, ab_train, L_transform=transforms.Normalize(mean=L_mean, std=L_std),\n",
    "                     ab_transform=transforms.Normalize(mean=ab_mean, std=ab_std))\n",
    "testset = MyDataset(L_test, ab_test, L_transform=transforms.Normalize(mean=L_mean, std=L_std),\n",
    "                     ab_transform=transforms.Normalize(mean=ab_mean, std=ab_std))\n",
    "del L_train, L_test, ab_train, ab_test # release memory"
   ],
   "id": "ba8dda1b447c7b6a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Early stopping used to stop training if model begins to overfit",
   "id": "6b19cc4864aa482e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def save_checkpoint(model, name='checkpoint'):\n",
    "    torch.save(model.state_dict(), f\"../models/{name}.pth\")\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, delta=5e-5):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_score = np.Inf\n",
    "        self.early_stop = False\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, net):\n",
    "        if self.best_score > val_loss:\n",
    "            self.best_score = val_loss\n",
    "            self.counter = 0\n",
    "            save_checkpoint(net)\n",
    "        elif self.best_score + self.delta < val_loss:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True"
   ],
   "id": "9c573fa9f9a004e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### ImageGradienLoss class and fit/predict methods\n",
    "\n",
    "$ \\_comp.pow(2).sum(dim=1)$ computes $‚àëùëê[Œî]^2$ directly, which is algebraically identical to $‚à•Œî‚à•_2^2$ but skips the square‚Äêroot."
   ],
   "id": "666dadc6d15b7c8f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class ImageGradientLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(ImageGradientLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        dh_in, dv_in = image_gradients(inputs) # (B, C, H, W-1), (B, C, H-1, W)\n",
    "        dh_tar, dv_tar = image_gradients(targets)\n",
    "        dh_comp = (dh_in - dh_tar) ** 2\n",
    "        dv_comp = (dv_in - dv_tar) ** 2\n",
    "        return (dh_comp + dv_comp).sum()\n",
    "\n",
    "def compute_pcc(pred, targets):\n",
    "    pred_flat = pred.reshape(pred.size(0), -1)\n",
    "    target_flat = targets.reshape(targets.size(0), -1)\n",
    "    vx = pred_flat - pred_flat.mean(dim=1, keepdim=True)\n",
    "    vy = target_flat - target_flat.mean(dim=1, keepdim=True)\n",
    "    numerator = (vx * vy).sum(dim=1) # covariance\n",
    "    denominator = torch.sqrt((vx**2).sum(dim=1) * (vy**2).sum(dim=1)) # standard deviation\n",
    "    return (numerator / denominator).mean()\n",
    "\n",
    "def fit(net, trainloader, optimizer, loss_fn1=nn.MSELoss(reduction='sum'), loss_fn2=ImageGradientLoss(), coeff=0.5):\n",
    "    net.train()\n",
    "    total_loss, total_rmse, total_psnr, total_ssim, total_pcc, count = 0, 0, 0, 0, 0, 0\n",
    "    for L, ab in trainloader:\n",
    "        L, ab = L.to(device), ab.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            out = net(L)\n",
    "            numel = ab.numel()\n",
    "            loss1, loss2 = loss_fn1(out, ab), loss_fn2(out, ab)\n",
    "            loss = (coeff * loss1 + (1-coeff) * loss2) / numel\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += loss.item()\n",
    "        count += len(ab)\n",
    "        with torch.no_grad():\n",
    "            batch_rmse = torch.sqrt(loss1 / numel)\n",
    "            batch_psnr = (20 * torch.log10(1.0 / batch_rmse))\n",
    "            total_rmse += batch_rmse.item() * len(ab)\n",
    "            total_psnr += batch_psnr.item() * len(ab)\n",
    "            total_ssim += structural_similarity_index_measure(out, ab).item() * len(ab)\n",
    "            total_pcc += compute_pcc(out, ab).item() * len(ab)\n",
    "        del out, L, ab, loss1, loss2, loss\n",
    "    return total_loss / count, total_rmse / count, total_psnr / count, total_ssim / count, total_pcc / count\n",
    "\n",
    "def predict(net, testloader, loss_fn1=nn.MSELoss(reduction='sum'), loss_fn2=ImageGradientLoss(), coeff=0.5):\n",
    "    net.eval()\n",
    "    total_loss, total_rmse, total_psnr, total_ssim, total_pcc, count = 0, 0, 0, 0, 0, 0\n",
    "    ins, preds, truths = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for L, ab in testloader:\n",
    "            L, ab = L.to(device), ab.to(device)\n",
    "            with torch.cuda.amp.autocast():\n",
    "                out = net(L)\n",
    "                ins.append(L.cpu())\n",
    "                preds.append(out.cpu())\n",
    "                truths.append(ab.cpu())\n",
    "                numel = out.numel()\n",
    "                loss1, loss2 = loss_fn1(out, ab), loss_fn2(out, ab)\n",
    "                loss = (coeff * loss1 + (1 - coeff) * loss2) / numel\n",
    "            total_loss += loss.item()\n",
    "            count += len(ab)\n",
    "            batch_rmse = torch.sqrt(loss1 / numel)\n",
    "            batch_psnr = (20 * torch.log10(1.0 / batch_rmse))\n",
    "            total_rmse += batch_rmse.item() * len(ab)\n",
    "            total_psnr += batch_psnr.item() * len(ab)\n",
    "            total_ssim += structural_similarity_index_measure(out, ab).item() * len(ab)\n",
    "            total_pcc += compute_pcc(out, ab).item() * len(ab)\n",
    "            del out, L, ab, loss1, loss2, loss\n",
    "    return ins, preds, truths, total_loss / count, total_rmse / count, total_psnr / count, total_ssim / count, total_pcc / count"
   ],
   "id": "90e8afa5dad977a9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Objective method for k fold cross validation",
   "id": "3ad383cdfdc0d080"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def objective(trial, trainset, X):\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-1, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [64,256, 512, 1024])\n",
    "    coeff = trial.suggest_float('coeff', 0.0, 1.0, log=False)\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    val_losses, mean_loss = [], 0\n",
    "    train_loss, train_rmse, train_psnr, train_ssim, train_pcc = 0, 0, 0, 0, 0\n",
    "    val_loss, val_rmse, val_psnr, val_ssim, val_pcc = 0, 0, 0, 0, 0\n",
    "    split_n = 0\n",
    "    prog_bar = tqdm(kf.split(X), desc=\"Splits\", position=0)\n",
    "    for train_idx, val_idx in prog_bar:\n",
    "        split_n += 1\n",
    "        trainloader = DataLoader(trainset, batch_size=batch_size, sampler=SubsetRandomSampler(train_idx), num_workers=4, pin_memory=True, prefetch_factor=2)\n",
    "        valloader = DataLoader(trainset, batch_size=batch_size, sampler=SubsetRandomSampler(val_idx), num_workers=4, pin_memory=True, prefetch_factor=2)\n",
    "        del train_idx, val_idx\n",
    "        net = Net().to(device)\n",
    "        optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.05, patience=2)\n",
    "        for epoch in range(50):\n",
    "            train_loss, train_rmse, train_psnr, train_ssim, train_pcc = fit(net, trainloader, optimizer, coeff=coeff)\n",
    "            ins, preds, truths, val_loss, val_rmse, val_psnr, val_ssim, val_pcc = predict(net, valloader, coeff=coeff)\n",
    "            del ins, preds, truths\n",
    "            scheduler.step(val_ssim)\n",
    "            prog_bar.set_description(\n",
    "                f\"Split {split_n} - Epoch {epoch + 1} |\\nlr={lr:.3e}, batch size={batch_size:.3e}, coeff={coeff:.3e} |\\n\"\n",
    "                f\"Metrics train/val: RMSE={train_rmse:.3e}/{val_rmse:.3e}, \"\n",
    "                f\"PSNR={train_psnr:.3e}/{val_psnr:.3e}, SSIM={train_ssim:.3e}/{val_ssim:.3e}, \"\n",
    "                f\"PCC={train_pcc:.3e}/{val_pcc:.3e} | Loss: {train_loss:.3e}/{val_loss:.3e}\")\n",
    "            torch.cuda.empty_cache()\n",
    "        del net, optimizer, scheduler\n",
    "        val_losses.append(val_loss)\n",
    "        mean_loss = np.mean(val_losses)\n",
    "        trial.report(mean_loss, split_n)\n",
    "        torch.cuda.empty_cache()\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "    return mean_loss"
   ],
   "id": "2a2eefa1ed92326e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### CAE NN definition\n",
    "\n",
    "out = (in - kernel + 2 * pad) / stride + 1"
   ],
   "id": "d4e41cba395737a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 128, 4, 2, 1)  # input is L only\n",
    "        self.conv2 = nn.Conv2d(128, 128, 4, 2, 1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, 4, 2, 1)\n",
    "        self.conv4 = nn.Conv2d(256, 512, 4, 2, 1)\n",
    "        self.conv5 = nn.Conv2d(512, 512, 4, 2, 1)\n",
    "\n",
    "        self.convt1 = nn.ConvTranspose2d(512, 512, 4, 2, 1)\n",
    "        self.convt2 = nn.ConvTranspose2d(1024, 256, 4, 2, 1)\n",
    "        self.convt3 = nn.ConvTranspose2d(512, 128, 4, 2, 1)\n",
    "        self.convt4 = nn.ConvTranspose2d(256, 128, 4, 2, 1)\n",
    "        self.convt5 = nn.ConvTranspose2d(256, 2, 4, 2, 1)  # output is ab (2 channels)\n",
    "\n",
    "        self.bnorm1 = nn.BatchNorm2d(256)\n",
    "        self.bnorm2 = nn.BatchNorm2d(512)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.lrelu = nn.LeakyReLU()\n",
    "        self.final = nn.Conv2d(3, 2, 1, 1)  # concat u5 + x = (2+1)=3 ‚Üí 2 (ab)\n",
    "\n",
    "    def forward(self, x): # x is (B, 1, H, W) => L channel\n",
    "        d1 = self.lrelu(self.conv1(x)) # (B, 128, 80, 80)\n",
    "        d2 = self.lrelu(self.conv2(d1)) # (B, 128, 40, 40)\n",
    "        d3 = self.lrelu(self.bnorm1(self.conv3(d2))) # (B, 256, 20, 20)\n",
    "        d4 = self.lrelu(self.bnorm2(self.conv4(d3))) # (B, 512, 10, 10)\n",
    "        d5 = self.lrelu(self.bnorm2(self.conv5(d4))) # (B, 512, 5, 5)\n",
    "        u1 = self.lrelu(self.convt1(d5)) # (B, 512, 10, 10)\n",
    "        u1 = torch.cat([u1, d4], dim=1) # (B, 1024, 10, 10)\n",
    "        u2 = self.lrelu(self.convt2(u1)) # (B, 256, 20, 20)\n",
    "        u2 = torch.cat([u2, d3], dim=1) # (B, 512, 20, 20)\n",
    "        u3 = self.lrelu(self.convt3(u2)) # (B, 128, 40, 40)\n",
    "        u3 = torch.cat([u3, d2], dim=1) # (B, 256, 40, 40)\n",
    "        u4 = self.lrelu(self.convt4(u3)) # (B, 128, 80, 80)\n",
    "        u4 = torch.cat([u4, d1], dim=1) # (B, 256, 80, 80)\n",
    "        u5 = self.lrelu(self.convt5(u4)) # (B, 2, 160, 160) ‚Äî ab prediction\n",
    "        u5 = torch.cat([u5, x], dim=1) # (B, 3, 160, 160)\n",
    "        x = self.final(self.dropout(u5)) # (B, 2, 160, 160)\n",
    "        return x # predicted ab"
   ],
   "id": "210bbe18343d7bf1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "writer = SummaryWriter('../runs')\n",
    "net = Net().to(device)\n",
    "writer.add_graph(net, torch.zeros(1, 1, SIZE, SIZE).to(device))\n",
    "writer.flush()\n",
    "summary(net, input_size=(1, 1, SIZE, SIZE), device=device)"
   ],
   "id": "52efb2f40d886975",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Run command:\\\n",
    "tensorboard --logdir=runs\\\n",
    "Visible at http://localhost:6006/"
   ],
   "id": "1fcb9650c3e809a1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Hyper parameter tuning",
   "id": "884003ec27deae8a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X = np.zeros(len(trainset))\n",
    "torch.cuda.empty_cache()\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "study = optuna.create_study(direction='minimize', pruner=optuna.pruners.MedianPruner())\n",
    "study.optimize(lambda trial: objective(trial, trainset, X), n_trials=5)"
   ],
   "id": "6308b5bea15ed4cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: \", trial.value)\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ],
   "id": "af4d5023becd2ee6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Entire dataset",
   "id": "71ef4a62e3f959ec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "def update_plot():\n",
    "    line1.set_data(range(len(train_losses)), train_losses)\n",
    "    line2.set_data(range(len(test_losses)), test_losses)\n",
    "    ax.relim()\n",
    "    ax.autoscale_view()\n",
    "    fig.canvas.draw()"
   ],
   "id": "b4a611dee10c7fa3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "trainloader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=4, pin_memory=True, prefetch_factor=2)\n",
    "testloader = DataLoader(testset, batch_size=128, shuffle=False, num_workers=4, pin_memory=True, prefetch_factor=2)\n",
    "optimizer = optim.Adam(net.parameters(), lr=5e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.05, patience=5)\n",
    "early_stopping = EarlyStopping()\n",
    "train_RMSEs, train_PSNRs, train_SSIMs, train_PCCs, train_losses = [], [], [], [], []\n",
    "test_RMSEs, test_PSNRs, test_SSIMs, test_PCCs, test_losses = [], [], [], [], []\n",
    "last_checkpoint = None\n",
    "prog_bar = tqdm(range(50), total=50, desc='Training', position=0)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "line1, = ax.plot([], [], label='Train Loss')\n",
    "line2, = ax.plot([], [], label='Test Loss')\n",
    "ax.legend()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "for epoch in prog_bar:\n",
    "    train_loss, train_RMSE, train_PSNR, train_SSIM, train_PCC = fit(net, trainloader, optimizer, coeff=0.35)\n",
    "    train_losses.append(train_loss)\n",
    "    train_RMSEs.append(train_RMSE)\n",
    "    train_PSNRs.append(train_PSNR)\n",
    "    train_SSIMs.append(train_SSIM)\n",
    "    train_PCCs.append(train_PCC)\n",
    "    ins, preds, truths, test_loss, test_RMSE, test_PSNR, test_SSIM, test_PCC = predict(net, testloader, coeff=0.35)\n",
    "    del ins, preds, truths\n",
    "    test_losses.append(test_loss)\n",
    "    test_RMSEs.append(test_RMSE)\n",
    "    test_PSNRs.append(test_PSNR)\n",
    "    test_SSIMs.append(test_SSIM)\n",
    "    test_PCCs.append(test_PCC)\n",
    "    scheduler.step(test_SSIM)\n",
    "    early_stopping(test_loss, net)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    prog_bar.set_description(f\"Epoch {epoch + 1} | lr={current_lr:.3e} |\\n\"\n",
    "                             f\"Metrics train/test: RMSE={train_RMSE:.3e}/{test_RMSE:.3e}, \"\n",
    "                             f\"PSNR={train_PSNR:.3e}/{test_PSNR:.3e}, SSIM={train_SSIM:.3e}/{test_SSIM:.3e}, \"\n",
    "                             f\"PCC={train_PCC:.3e}/{test_PCC:.3e} | Loss: {train_loss:.3e}/{test_loss:.3e}\")\n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "    writer.add_scalar('Loss/test', test_loss, epoch)\n",
    "    update_plot()\n",
    "    torch.cuda.empty_cache()\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        # break\n",
    "save_checkpoint(net, 'lastcheck')\n",
    "writer.flush()"
   ],
   "id": "d3f1531e5ebfb847",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluate results",
   "id": "5ef496b060c5cef7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "net.load_state_dict(torch.load('../models/checkpoint.pth'))\n",
    "ins, preds, truths, test_loss, test_RMSE, test_PSNR, test_SSIM, test_PCC = predict(net, testloader)\n",
    "net.load_state_dict(torch.load('../models/lastcheck.pth'))\n",
    "ins2, preds2, truths2, loss2, rmse2, psnr2, ssim2, pcc2 = predict(net, testloader)"
   ],
   "id": "474e9b89f3c6b42f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def unstandardize(tensor, mean, std):\n",
    "    unnorm = transforms.Normalize(mean=[-m/s for m, s in zip(mean, std)], std=[1/s for s in std])\n",
    "    return unnorm(tensor).clamp(0, 1)\n",
    "\n",
    "def lab_to_rgb(x):\n",
    "    lab = x.permute(1, 2, 0).cpu().numpy()\n",
    "    L = (lab[:, :, 0] * 255).astype(np.uint8)\n",
    "    a = (lab[:, :, 1] * 255).astype(np.uint8)\n",
    "    b = (lab[:, :, 2] * 255).astype(np.uint8)\n",
    "    lab_cv = np.stack([L, a, b], axis=2)\n",
    "    rgb = cv2.cvtColor(lab_cv, cv2.COLOR_LAB2RGB)\n",
    "    return rgb"
   ],
   "id": "d7fb7f5f4419195b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ins = torch.cat(ins, dim=0)\n",
    "preds = torch.cat(preds, dim=0)\n",
    "truths = torch.cat(truths, dim=0)\n",
    "ins = [unstandardize(x, L_mean, L_std)for x in ins]\n",
    "preds = [unstandardize(x, ab_mean, ab_std)for x in preds]\n",
    "truths = [unstandardize(x, ab_mean, ab_std) for x in truths]\n",
    "preds_rgb = []\n",
    "truths_rgb = []\n",
    "for L, ab_pred, ab_true in zip(ins, preds, truths):\n",
    "    lab_pred = torch.cat([L, ab_pred], dim=0)  # [3, H, W]\n",
    "    lab_true = torch.cat([L, ab_true], dim=0)\n",
    "    preds_rgb.append(lab_to_rgb(lab_pred))\n",
    "    truths_rgb.append(lab_to_rgb(lab_true))\n",
    "preds = preds_rgb\n",
    "truths = truths_rgb\n",
    "ins2 = torch.cat(ins2, dim=0)\n",
    "preds2 = torch.cat(preds2, dim=0)\n",
    "truths2 = torch.cat(truths2, dim=0)\n",
    "ins2 = [unstandardize(x, L_mean, L_std)for x in ins2]\n",
    "preds2 = [unstandardize(x, ab_mean, ab_std)for x in preds2]\n",
    "truths2 = [unstandardize(x, ab_mean, ab_std) for x in truths2]\n",
    "preds_rgb = []\n",
    "truths_rgb = []\n",
    "for L, ab_pred, ab_true in zip(ins2, preds2, truths2):\n",
    "    lab_pred = torch.cat([L, ab_pred], dim=0)\n",
    "    lab_true = torch.cat([L, ab_true], dim=0)\n",
    "    preds_rgb.append(lab_to_rgb(lab_pred))\n",
    "    truths_rgb.append(lab_to_rgb(lab_true))\n",
    "preds2 = preds_rgb\n",
    "truths2 = truths_rgb"
   ],
   "id": "a9eb5233d4636019",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(test_loss, test_RMSE, test_PSNR, test_SSIM, test_PCC)\n",
    "print(loss2, rmse2, psnr2, ssim2, pcc2)"
   ],
   "id": "84e96a7851eeb43",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_RMSEs, label='Train RMSE')\n",
    "plt.plot(test_RMSEs, label='Test RMSE')\n",
    "plt.axhline(y=test_RMSE, color='g', linestyle='--')\n",
    "plt.axhline(y=rmse2, color='r', linestyle='--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_PSNRs, label='Train PSNR')\n",
    "plt.plot(test_PSNRs, label='Test PSNR')\n",
    "plt.axhline(y=test_PSNR, color='g', linestyle='--')\n",
    "plt.axhline(y=psnr2, color='r', linestyle='--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('PSNR')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_SSIMs, label='Train SSIM')\n",
    "plt.plot(test_SSIMs, label='Test SSIM')\n",
    "plt.axhline(y=test_SSIM, color='g', linestyle='--')\n",
    "plt.axhline(y=ssim2, color='r', linestyle='--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('SSIM')\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_PCCs, label='Train PCC')\n",
    "plt.plot(test_PCCs, label='Test PCC')\n",
    "plt.axhline(y=test_PCC, color='g', linestyle='--')\n",
    "plt.axhline(y=pcc2, color='r', linestyle='--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('PCC')\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_losses, label='Train loss')\n",
    "plt.plot(test_losses, label='Test loss')\n",
    "plt.axhline(y=test_loss, color='g', linestyle='--')\n",
    "plt.axhline(y=loss2, color='r', linestyle='--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim(bottom=0)\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "c95813c83890212e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for _ in range(5):\n",
    "    idx = np.random.randint(0, len(ins))\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title('Gray Image', fontsize=20)\n",
    "    plt.imshow(ins[idx].squeeze().cpu().numpy() , cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title('Predicted Image', fontsize=20)\n",
    "    plt.imshow(preds[idx])  # Already a [H, W, 3] NumPy RGB image\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.title('Groundtruth Image', fontsize=20)\n",
    "    plt.imshow(truths[idx])  # Already a [H, W, 3] NumPy RGB image\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ],
   "id": "7461edeb4ce4843e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "535b176866a8bf8d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
